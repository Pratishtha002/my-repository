{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOojle3LBM4xPl9VtpdhN+H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# =========================\n","# Step 0. Install packages\n","# =========================\n","!pip install -q chromadb gradio PyPDF2 reportlab python-docx matplotlib nltk sentence-transformers transformers accelerate pymupdf beautifulsoup4 requests openai\n","\n","# =========================\n","# Step 1. Imports\n","# =========================\n","import os, re, uuid, tempfile, traceback, requests\n","import nltk\n","nltk.download(\"punkt\", quiet=True)\n","\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import gradio as gr\n","from PyPDF2 import PdfReader\n","import fitz  # PyMuPDF\n","from bs4 import BeautifulSoup\n","\n","# Embeddings + Local Models\n","from sentence_transformers import SentenceTransformer\n","from transformers import pipeline\n","from openai import OpenAI\n","\n","# ChromaDB setup\n","import chromadb\n","from chromadb.config import Settings\n","\n","# =========================\n","# Step 2. Initialize DB + Models\n","# =========================\n","CHROMA_DIR = \"./chroma_store\"\n","os.makedirs(CHROMA_DIR, exist_ok=True)\n","\n","try:\n","    chroma_client = chromadb.Client(\n","        Settings(chroma_db_impl=\"duckdb+parquet\", persist_directory=CHROMA_DIR)\n","    )\n","except Exception:\n","    chroma_client = chromadb.Client()\n","\n","try:\n","    collection = chroma_client.create_collection(\n","        name=\"docs\", metadata={\"hnsw:space\": \"cosine\"}, get_or_create=True\n","    )\n","except TypeError:\n","    try:\n","        collection = chroma_client.get_collection(\"docs\")\n","    except Exception:\n","        collection = chroma_client.create_collection(\n","            name=\"docs\", metadata={\"hnsw:space\": \"cosine\"}\n","        )\n","\n","embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","# =========================\n","# Step 3. Chat Models\n","# =========================\n","from openai import OpenAI\n","from huggingface_hub import InferenceClient\n","\n","# --- OpenAI (GPT) ---\n","OPENAI_KEY = \"openai key "\n","\n","if OPENAI_KEY:\n","    client = OpenAI(api_key=OPENAI_KEY)\n","    model_provider = \"openai\"\n","    print(\"‚úÖ Using OpenAI GPT\")\n","else:\n","    client = InferenceClient(\"tiiuae/falcon-7b-instruct\")  # HuggingFace fallback\n","    model_provider = \"falcon\"\n","    print(\"‚ö†Ô∏è No OpenAI key found ‚Üí Using Falcon fallback\")\n","\n","chat_history = []\n","all_texts = []\n","\n","\n","\n","# =========================\n","# Step 4. Helpers\n","# =========================\n","def normalize_paths(files):\n","    paths = []\n","    for f in files or []:\n","        if isinstance(f, str):\n","            paths.append(f)\n","        else:\n","            p = getattr(f, \"name\", None)\n","            if p:\n","                paths.append(p)\n","    return paths\n","\n","def extract_text(filepath: str) -> str:\n","    text = \"\"\n","    try:\n","        if filepath.lower().endswith(\".txt\"):\n","            with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fp:\n","                text = fp.read()\n","        elif filepath.lower().endswith(\".pdf\"):\n","            try:\n","                reader = PdfReader(filepath)\n","                for page in reader.pages:\n","                    extracted = page.extract_text() or \"\"\n","                    text += extracted + \"\\n\"\n","            except Exception:\n","                pass\n","            if len(text.strip()) < 50:\n","                try:\n","                    with fitz.open(filepath) as doc:\n","                        for pg in doc:\n","                            text += pg.get_text() or \"\"\n","                except Exception:\n","                    pass\n","    except Exception as e:\n","        print(f\"[extract_text] ERROR on {filepath}: {e}\")\n","    return text.strip()\n","\n","def extract_url_text(url: str) -> str:\n","    \"\"\"Scrape main text from webpage\"\"\"\n","    try:\n","        resp = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n","        soup = BeautifulSoup(resp.text, \"html.parser\")\n","        for script in soup([\"script\", \"style\"]):\n","            script.decompose()\n","        text = \" \".join(t.strip() for t in soup.stripped_strings)\n","        return text[:20000]  # limit\n","    except Exception as e:\n","        return f\"‚ö†Ô∏è Could not fetch URL: {e}\"\n","\n","def chunk_text(s: str, chunk_size=1200, overlap=200):\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    chunks, start, n = [], 0, len(s)\n","    while start < n:\n","        end = min(start + chunk_size, n)\n","        chunks.append(s[start:end])\n","        if end == n: break\n","        start = end - overlap\n","    return chunks\n","\n","def collection_count_safe(col):\n","    try:\n","        return col.count()\n","    except Exception:\n","        try:\n","            return len(col.get()[\"ids\"])\n","        except Exception:\n","            return 0\n","\n","# =========================\n","# Step 5. Index Files or URLs\n","# =========================\n","def index_files(files):\n","    global all_texts\n","    try:\n","        paths = normalize_paths(files)\n","        if not paths:\n","            return \"‚ö†Ô∏è Please upload at least one file.\"\n","        added = 0\n","        for path in paths:\n","            text = extract_text(path)\n","            if not text or len(text.strip()) < 20:\n","                return f\"‚ö†Ô∏è No readable text in {os.path.basename(path)}\"\n","            chunks = chunk_text(text)\n","            embeddings = embedder.encode(chunks, convert_to_numpy=False)\n","            ids = [str(uuid.uuid4()) for _ in chunks]\n","            collection.add(documents=chunks,\n","                           embeddings=[e.tolist() for e in embeddings],\n","                           metadatas=[{\"filename\": os.path.basename(path)}]*len(chunks),\n","                           ids=ids)\n","            all_texts.append(text)\n","            added += 1\n","        return f\"‚úÖ Indexed {added} file(s).\"\n","    except Exception as e:\n","        return f\"‚ùå Indexing Failed: {str(e)}\"\n","\n","def index_url(url):\n","    global all_texts\n","    text = extract_url_text(url)\n","    if not text or \"‚ö†Ô∏è\" in text:\n","        return text\n","    chunks = chunk_text(text)\n","    embeddings = embedder.encode(chunks, convert_to_numpy=False)\n","    ids = [str(uuid.uuid4()) for _ in chunks]\n","    collection.add(documents=chunks,\n","                   embeddings=[e.tolist() for e in embeddings],\n","                   metadatas=[{\"source\": url}]*len(chunks),\n","                   ids=ids)\n","    all_texts.append(text)\n","    return f\"‚úÖ Indexed content from URL: {url}\"\n","\n","# =========================\n","# Step 6. Reset DB\n","# =========================\n","def reset_collection():\n","    global collection, chat_history, all_texts\n","    try:\n","        chroma_client.delete_collection(\"docs\")\n","    except Exception:\n","        pass\n","    try:\n","        collection = chroma_client.create_collection(\n","            name=\"docs\", metadata={\"hnsw:space\": \"cosine\"}, get_or_create=True\n","        )\n","    except TypeError:\n","        collection = chroma_client.create_collection(\n","            name=\"docs\", metadata={\"hnsw:space\": \"cosine\"}\n","        )\n","    chat_history, all_texts = [], []\n","    return \"üóëÔ∏è All documents cleared!\"\n","\n","# =========================\n","# Step 7. RAG + Generative Answers\n","# =========================\n","def answer_question(message, history):\n","    try:\n","        if collection_count_safe(collection) == 0:\n","            return \"‚ö†Ô∏è No documents indexed yet.\"\n","        q_vec = embedder.encode(message).tolist()\n","        res = collection.query(query_embeddings=[q_vec], n_results=3)\n","        docs = res.get(\"documents\", [[]])[0]\n","        context = \"\\n\".join(docs)[:4000]\n","\n","        prompt = f\"You are a helpful AI assistant. Use the following context to answer the user:\\n\\nContext:\\n{context}\\n\\nQuestion: {message}\\nAnswer in a clear and natural way:\"\n","\n","        if client:  # OpenAI mode\n","            resp = client.chat.completions.create(\n","                model=\"gpt-4o-mini\",\n","                messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","                          {\"role\": \"user\", \"content\": prompt}],\n","                max_tokens=400\n","            )\n","            answer = resp.choices[0].message.content.strip()\n","        else:  # Local fallback\n","            gen = gen_pipeline(prompt, max_new_tokens=200, do_sample=True, temperature=0.7)\n","            answer = gen[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n","\n","        chat_history.append({\"Q\": message, \"A\": answer})\n","        return answer\n","    except Exception as e:\n","        return f\"‚ùå Chat Error: {str(e)}\"\n","\n","# =========================\n","# Step 8. Gradio UI\n","# =========================\n","with gr.Blocks(theme=\"soft\") as demo:\n","    gr.Markdown(\"## üìö Internal AI Assistant (Enhanced + GPT Option)\")\n","\n","    with gr.Row():\n","        file_upload = gr.Files(label=\"üìÇ Upload TXT/PDF Files\", file_count=\"multiple\", type=\"filepath\")\n","        url_input = gr.Textbox(label=\"üîó Paste URL to Index\")\n","\n","    with gr.Row():\n","        upload_btn = gr.Button(\"üì• Index Files\")\n","        url_btn = gr.Button(\"üåê Index URL\")\n","        reset_btn = gr.Button(\"üóëÔ∏è Reset Database\")\n","\n","    upload_output = gr.Textbox(label=\"File Upload Status\", interactive=False)\n","    url_output = gr.Textbox(label=\"URL Upload Status\", interactive=False)\n","    reset_output = gr.Textbox(label=\"Reset Status\", interactive=False)\n","\n","    upload_btn.click(fn=index_files, inputs=file_upload, outputs=upload_output)\n","    url_btn.click(fn=index_url, inputs=url_input, outputs=url_output)\n","    reset_btn.click(fn=reset_collection, outputs=reset_output)\n","\n","    gr.Markdown(\"### üí¨ Chat with your Docs / URLs\")\n","    gr.ChatInterface(fn=answer_question)\n","\n","# =========================\n","# Step 9. Launch\n","# =========================\n","if __name__ == \"__main__\":\n","    demo.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":698},"id":"4Tiq47VKGlJI","executionInfo":{"status":"ok","timestamp":1756024849341,"user_tz":-330,"elapsed":27332,"user":{"displayName":"Pratishtha Jagtap","userId":"12325670074337303170"}},"outputId":"f2c3d7d5-00b6-4f77-d330-bbab96bd836b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Using OpenAI GPT\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n","  self.chatbot = Chatbot(\n"]},{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://8a90b4dfe10ab85922.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://8a90b4dfe10ab85922.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}]}]}
